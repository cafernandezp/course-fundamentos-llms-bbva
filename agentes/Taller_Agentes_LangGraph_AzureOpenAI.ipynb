{"cells":[{"cell_type":"markdown","metadata":{"id":"vjaszwJM-v4b"},"source":["## Taller de agentes con LangGraph\n","\n","En este taller práctico, crearemos un agente usando la librería LangGraph y modelos de OpenAI. Este agente tendrá dos herramientas a su disposición:\n","\n","*   **Tavily**: para búsquedas web\n","*   **WeatherAPI**: para previsión del tiempo (o tiempo actual)\n","\n","El agente detectará la intencionalidad de la pregunta, decidirá si debe usar alguna de estas herramientas, y devolverá una respuesta al usuario construida a partir de la información recuperada de estas APIs"]},{"cell_type":"markdown","metadata":{"id":"xrrTCxUh2PgG"},"source":["### Instalamos las librerías necesarias: paquetes específicos de langchain"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"92RTnCT3zItu"},"outputs":[],"source":["!pip install -q langgraph langchain langchain-community langchain-openai python-dotenv"]},{"cell_type":"markdown","metadata":{"id":"mft3iRt32Via"},"source":["### Importamos las herramientas necesarias, que todas están integradas en langchain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kORuDqqJ0heh"},"outputs":[],"source":["import os\n","import requests\n","from typing import List, Literal\n","from langchain_community.tools.tavily_search import TavilySearchResults\n","from langchain_core.tools import tool\n","from langchain_openai import AzureChatOpenAI"]},{"cell_type":"markdown","metadata":{"id":"0XWwoAVu2b3w"},"source":["### Definimos las APIs necesarias, tanto de OpenAI como de WeatherAPI\n","\n","Recordemos que en una aplicación real, las APIs no deben quedar expuestas en el código"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_haI6Zjn0WR3"},"outputs":[],"source":["from google.colab import userdata\n","OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n","API_VERSION = userdata.get('OPENAI_API_VERSION')\n","AZURE_ENDPOINT = userdata.get('AZURE_OPENAI_ENDPOINT')\n","WEATHER_API_KEY = userdata.get('WEATHER_API_KEY')\n","TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n","os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY"]},{"cell_type":"markdown","metadata":{"id":"Va2B0_Wg2k1B"},"source":["### Empezamos definiendo las tools (herramientas) que va a tener a su disposición el agente\n","\n","Se usa el decorador @tool para indicarle a langchain que son herramientas. Al incluir este decorador, se extraen metadatos importantes como el nombre de la herramienta (que por defecto es el nombre de la función) y la descripción (el docstring de la función, la primera línea que se define entre comillas)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-eM8WFe0YHp"},"outputs":[],"source":["@tool\n","def get_weather(query: str) -> list:\n","    \"\"\"Search weatherapi to get the current or forecast weather\"\"\" #docstring para darle la descripción\n","    endpoint = f\"http://api.weatherapi.com/v1/current.json?key={WEATHER_API_KEY}&q={query}\"\n","    response = requests.get(endpoint)\n","    data = response.json()\n","\n","    if data.get(\"location\"):\n","        return data\n","    else:\n","        return \"Weather Data Not Found\"\n","\n","@tool\n","def search_web(query: str) -> list:\n","    \"\"\"Search the web for a query\"\"\" #docstring para darle la descripción\n","    tavily_search = TavilySearchResults(max_results=5, search_depth='advanced', max_tokens=1000)\n","    results = tavily_search.invoke(query)\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"6O8VIX2S20jJ"},"source":["### Probamos manualmente la API de get_weather"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"W7dPxREa23td"},"outputs":[],"source":["get_weather(\"Madrid\")"]},{"cell_type":"markdown","metadata":{"id":"3Nrz7EoLCXn1"},"source":["### Probamos manualmente la API de Tavily"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cob3_ckyCXSw"},"outputs":[],"source":["search_web(\"¿cuando inicia la temporada de la NFL 2026?\")"]},{"cell_type":"markdown","metadata":{"id":"KKcB9bk23lcU"},"source":["### Llamamos al modelo con el parámetro bind_tools(), que permite pasar tools de Langchain para que las use el LLM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SvN2get80nQS"},"outputs":[],"source":["os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_ENDPOINT\n","\n","llm = AzureChatOpenAI(azure_deployment=\"gpt-4o\",\n","    api_key = OPENAI_API_KEY,\n","    api_version=API_VERSION,\n","    temperature=0.1,\n","    max_tokens=None,\n","    timeout=None,\n","    max_retries=2,)\n","\n","tools = [search_web, get_weather]\n","llm_with_tools = llm.bind_tools(tools)"]},{"cell_type":"markdown","metadata":{"id":"ETcbdy7Y4rx3"},"source":["### El LLM por si mismo no tiene acceso a información en tiempo real"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RVcg3sAh4sYo"},"outputs":[],"source":["query = \"What is the current weather in Segovia today?\"\n","response = llm.invoke(query)\n","print(response.content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7OtZHegPHoGo"},"outputs":[],"source":["query = \"quien fue el campeon del super bowl en 2025?\"\n","response = llm.invoke(query)\n","print(response.content)"]},{"cell_type":"markdown","metadata":{"id":"XRTkSiF34SWk"},"source":["### Podemos definir el prompt que se le pasará al LLM+tools, explicándole la tarea o tareas que queremos que haga"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xHr5RVtV0zaG"},"outputs":[],"source":["prompt = \"\"\"\n","    Given the tools at your disposal, mention tool calls for the following tasks --If any answer can´t be provided from tool calls, use your own knowledge:\n","    Do not change the query given for any search tasks\n","        1. What is the current weather in Madrid today\n","        2. Can you tell me about Kerala\n","        3. Why is the sky blue?\n","    \"\"\"\n","\n","results = llm_with_tools.invoke(prompt)\n","\n","print(results.tool_calls)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1G1stDv5CE6"},"outputs":[],"source":["print(results)"]},{"cell_type":"code","source":["results.content"],"metadata":{"id":"fafvLBuml1m2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NBEnkV1U5LZV"},"source":["### Ahora creamos el agente con grafos, usando LangGraph\n","\n","En primer lugar, usamos el módulo create_react_agent. El paradigma ReAct está bastado en Reason & Act, es decir, razonar (¿qué tool tengo a mi disposición y cuál debería usar?) y actuar (voy a usar esta herramienta para que me devuelva una respuesta y analizarla)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K1FDhsxU5mmK"},"outputs":[],"source":["from langgraph.prebuilt import create_react_agent"]},{"cell_type":"markdown","metadata":{"id":"SSrBc1iF5vzB"},"source":["Creamos un system_prompt, donde le explicamos qué esperamos que haga, y las tools que tiene a su disposición. Es importante que los nombres de las tools coincidan con el nombre con el que le hemos definido, así como darle una buena explicación de su función"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P_0-Y0ic5vkb"},"outputs":[],"source":["system_prompt = \"\"\"Act as a helpful assistant.\n","    Use the tools at your disposal to perform tasks as needed.\n","        - get_weather: whenever user asks get the weather of a place.\n","        - search_web: whenever user asks for information on current events or if you don't know the answer.\n","    Use the tools only if you don't know the answer.\"\"\""]},{"cell_type":"markdown","metadata":{"id":"_ftpBo8b8A5N"},"source":["### Inicializamos el agente con el modelo, las tools y el system_prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LWYVYttDXGp5"},"outputs":[],"source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","prompt_template = ChatPromptTemplate.from_messages([(\"system\", system_prompt), (\"placeholder\", \"{messages}\")])\n","\n","agent = create_react_agent(model=llm, tools=tools, prompt=prompt_template)"]},{"cell_type":"markdown","metadata":{"id":"FTSYMgX28G9V"},"source":["### Probamos el agente para ver qué hace"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wnq3-VSB02Hs"},"outputs":[],"source":["def print_stream(stream):\n","    for s in stream:\n","        message = s[\"messages\"][-1]\n","        if isinstance(message, tuple):\n","            print(message)\n","        else:\n","            message.pretty_print()\n","\n","inputs = {\"messages\": [(\"user\", \"What is the current weather in Madrid today\")]}\n","\n","print_stream(agent.stream(inputs, stream_mode=\"values\"))"]},{"cell_type":"markdown","metadata":{"id":"-GMrcolI8nnw"},"source":["### Vamos a crear la lógica de grafos (estados y nodos)\n","\n","En primer lugar, definimos el conjunto de tools posibles mediante el ToolNode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5i8-N_W78ySE"},"outputs":[],"source":["from langgraph.prebuilt import ToolNode\n","from langgraph.graph import StateGraph, MessagesState, START, END\n","\n","tools = [search_web, get_weather]\n","tool_node = ToolNode(tools)"]},{"cell_type":"markdown","metadata":{"id":"ba3HR2MZ82aD"},"source":["Definimos las funciones para llamar al LLM y las tools. La función call_tools llevará al final (END)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tCCA2FRT8_HI"},"outputs":[],"source":["def call_model(state: MessagesState):\n","    messages = state[\"messages\"]\n","    response = llm_with_tools.invoke(messages)\n","    return {\"messages\": [response]}\n","\n","def call_tools(state: MessagesState) -> Literal[\"tools\", END]:\n","    messages = state[\"messages\"]\n","    last_message = messages[-1]\n","    if last_message.tool_calls:\n","        return \"tools\"\n","    return END"]},{"cell_type":"markdown","metadata":{"id":"uo6J8mcj9huR"},"source":["Inicializamos el flow con StateGraph"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIX-4a2X9krQ"},"outputs":[],"source":["workflow = StateGraph(MessagesState)"]},{"cell_type":"markdown","metadata":{"id":"rob7UP8u9mgb"},"source":["Ahora ya podemos añadir nodos. Primero, añadimos un nodo de LLM, que usará el LLM para tomar decisiones en base al input"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmBLCY5c9xVa"},"outputs":[],"source":["workflow.add_node(\"LLM\", call_model)"]},{"cell_type":"markdown","metadata":{"id":"QOJke0Sn9yAX"},"source":["Nuestro flow comienza con el nodo LLM. Esto se codifica añadiendolo con la función add_edge, indicando que es START"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Po3T-HcU-j5T"},"outputs":[],"source":["workflow.add_edge(START, \"LLM\")"]},{"cell_type":"markdown","metadata":{"id":"36-XUcjm-lHH"},"source":["Después, pasamos por el nodo de las tools"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBmAqy3L-qb5"},"outputs":[],"source":["workflow.add_node(\"tools\", tool_node)"]},{"cell_type":"markdown","metadata":{"id":"DJymonf--oOh"},"source":["Añadimos un edge condicional, que va del LLM a las llamadas de las tools. Dependiendo del output del LLM, irá al nodo de tools o al final (END), según el agente considere que necesita consultar una de las tools o puede dar una respuesta autónoma"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LgO9GjMC-622"},"outputs":[],"source":["workflow.add_conditional_edges(\"LLM\", call_tools)"]},{"cell_type":"markdown","metadata":{"id":"o3jLc5S--8W5"},"source":["Por último, el nodo tools le manda la información de vuelta al LLM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BKz4909N_HPR"},"outputs":[],"source":["workflow.add_edge(\"tools\", \"LLM\")"]},{"cell_type":"markdown","metadata":{"id":"8LU9Xq3T_K7Y"},"source":["Cuando hemos definido todo nuestro flow, se debe compilar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rqliEtxs_STZ"},"outputs":[],"source":["agent = workflow.compile()"]},{"cell_type":"markdown","metadata":{"id":"pSdOlzPw_Tm7"},"source":["Se puede pintar el flow automáticamente"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYhf7PDm_Xpc"},"outputs":[],"source":["from IPython.display import Image, display\n","\n","display(Image(agent.get_graph().draw_mermaid_png()))"]},{"cell_type":"markdown","metadata":{"id":"f9G5m_bQ_mdB"},"source":["### Ya podemos usar nuestro agente y ver los pasos intermedios que sigue"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tITkEgsY07zM"},"outputs":[],"source":["for chunk in agent.stream(\n","    {\"messages\": [(\"user\", \"What was the stock price of BBVA in the Spanish market as of March 6th, 2025? And will I need an umbrella in Madrid for tomorrow? y quien fue el campeon del super bowl 2025?\")]}, #\"Will I need an umbrella in Madrid tomorrow?\"\n","    stream_mode=\"values\",):\n","    chunk[\"messages\"][-1].pretty_print()"]},{"cell_type":"code","source":[],"metadata":{"id":"M6cu_f3fpGnf"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["xrrTCxUh2PgG"]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}