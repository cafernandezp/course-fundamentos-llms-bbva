{"cells":[{"cell_type":"markdown","metadata":{"id":"J5WLhsu8WN3K"},"source":["## Ejercicio 8: Langchain\n","En este ejercicio, vemos un par de ejemplos con LangChain. Primero, refinaremos un prompt, y después lo usaremos para crear un resumen"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ibt4exKXpyC"},"outputs":[],"source":["from google.colab import userdata\n","OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n","API_VERSION = userdata.get('OPENAI_API_VERSION')\n","AZURE_ENDPOINT = userdata.get('AZURE_OPENAI_ENDPOINT')\n","\n","print(type(OPENAI_API_KEY),OPENAI_API_KEY)\n","print(type(API_VERSION),API_VERSION)\n","print(type(AZURE_ENDPOINT),AZURE_ENDPOINT)"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1759814020941,"user":{"displayName":"VICTOR VALLEJO CARBALLO","userId":"16397145822283464326"},"user_tz":-120},"id":"LRhjVGwNWN3L"},"outputs":[],"source":["import os"]},{"cell_type":"markdown","metadata":{"id":"aPnYpbdeWN3M"},"source":["##### Establecemos el valor de la API key de OpenAI. En una aplicación real, no podemos exponer esta API key en el código, y la cogeríamos con una variable de entorno mediante un os.getenv()"]},{"cell_type":"markdown","metadata":{"id":"LOhVvNJUWN3M"},"source":["##### Importamos langchain\n","Si no está instalado, descomenta la próxima línea"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMkKwZy9WN3N"},"outputs":[],"source":["!pip -q install langchain\n","!pip -q install langchain-openai"]},{"cell_type":"markdown","metadata":{"id":"gAZMTDBDWN3N"},"source":["##### Primero, definimos dos LLMs, uno para refinar un prompt (tarea creativa) y el segundo para utilizar dicho prompt (tarea poco creativa)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2746,"status":"ok","timestamp":1759814063675,"user":{"displayName":"VICTOR VALLEJO CARBALLO","userId":"16397145822283464326"},"user_tz":-120},"id":"IbKLRzXGWN3N"},"outputs":[],"source":["from langchain import PromptTemplate, LLMChain\n","from langchain_openai import AzureChatOpenAI\n","\n","os.environ[\"AZURE_OPENAI_ENDPOINT\"] = AZURE_ENDPOINT\n","\n","llm_refine = AzureChatOpenAI(azure_deployment=\"gpt-4o\",\n","    api_key = OPENAI_API_KEY,\n","    api_version=API_VERSION,\n","    temperature=0.7,\n","    max_tokens=None,\n","    timeout=None,\n","    max_retries=2,)\n","\n","llm_summary = AzureChatOpenAI(azure_deployment=\"gpt-4o\",\n","    api_key = OPENAI_API_KEY,\n","    api_version=API_VERSION,\n","    temperature=0.1,\n","    max_tokens=None,\n","    timeout=None,\n","    max_retries=2,)"]},{"cell_type":"markdown","metadata":{"id":"UGE3UxjMWN3N"},"source":["##### Definimos un prompt base para generar resúmenes, y su versión para ser refinado"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1759814068697,"user":{"displayName":"VICTOR VALLEJO CARBALLO","userId":"16397145822283464326"},"user_tz":-120},"id":"HNhBwUXrWN3N"},"outputs":[],"source":["base_summary_prompt = \"\"\"\n","Eres un asistente experto en síntesis de información.\n","Tu tarea es leer el siguiente texto y generar un resumen que destaque las ideas principales.\n","Asegúrate de:\n","- Identificar los puntos clave.\n","- Omitir detalles secundarios.\n","- Mantener la coherencia y cohesión en el resumen.\n","\n","Texto:\n","{text}\n","\n","Resumen:\"\"\"\n","\n","\n","refinement_template = \"\"\"\n","Eres un experto en ingeniería de prompts.\n","Reformula, mejora y amplía con mucho más detalle el siguiente prompt de resumen para que sea más claro, específico y efectivo. El texto a resumir será del dominio de las finanzas y banca:\n","Prompt original:\n","{base_prompt}\n","\n","Devuelve UNICAMENTE el prompt refinado, sin más texto antes o después.\n","\n","Prompt refinado:\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"JQkPB7HwWN3O"},"source":["##### Ahora usamos el módulo PromptTemplate de Langchain, y LLMChain para usar programáticamente dicho template con el LLM que hemos definido antes\n","\n","Hasta que no ejecutamos el comando \"run\", no está llamando al LLM, solo definiendo las instrucciones (como en Spark)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FK5h4QGXWN3O"},"outputs":[],"source":["from IPython.display import Markdown\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"langchain.*\")\n","\n","refine_prompt_template = PromptTemplate(\n","    template=refinement_template,\n","    input_variables=[\"base_prompt\"]\n",")\n","refine_chain = LLMChain(llm=llm_refine, prompt=refine_prompt_template)\n","\n","refined_prompt = refine_chain.run(base_prompt=base_summary_prompt)\n","print(\"Prompt refinado:\\n\")\n","display(Markdown(refined_prompt))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1759814103208,"user":{"displayName":"VICTOR VALLEJO CARBALLO","userId":"16397145822283464326"},"user_tz":-120},"id":"ZgAiN_D97UV8","outputId":"4c727762-de8f-48b3-b90c-fc61e5bd26b1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["PromptTemplate(input_variables=['base_prompt'], input_types={}, partial_variables={}, template='\\nEres un experto en ingeniería de prompts.\\nReformula, mejora y amplía con mucho más detalle el siguiente prompt de resumen para que sea más claro, específico y efectivo. El texto a resumir será del dominio de las finanzas y banca:\\nPrompt original:\\n{base_prompt}\\n\\nDevuelve UNICAMENTE el prompt refinado, sin más texto antes o después.\\n\\nPrompt refinado:\\n')"]},"metadata":{},"execution_count":8}],"source":["refine_prompt_template"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cOHsUQLXZU4I"},"outputs":[],"source":["from IPython.display import Markdown\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"langchain.*\")\n","\n","\n","# Crear el prompt template\n","refine_prompt_template = PromptTemplate(\n","   template=refinement_template,\n","   input_variables=[\"base_prompt\"]\n",")\n","\n","\n","# Crear la cadena LLM con tu modelo\n","refine_chain = LLMChain(llm=llm_refine, prompt=refine_prompt_template)\n","\n","\n","# Ejecutar correctamente con invoke (nuevo estándar porque en versiones anteriores run() esta deprecado)\n","refined_prompt = refine_chain.invoke({\"base_prompt\": base_summary_prompt})\n","\n","\n","# Inspeccionar o extraer resultado (ajusta si la clave es diferente)\n","refined_prompt_str = refined_prompt[\"text\"] if \"text\" in refined_prompt else refined_prompt[\"base_prompt\"]\n","\n","\n","# Mostrar el resultado con estilo Markdown\n","print(\"Prompt refinado:\\n\")\n","display(Markdown(refined_prompt_str))\n"]},{"cell_type":"markdown","metadata":{"id":"-Y6U84sfWN3O"},"source":["##### Ahora, creamos la cadena de resumen usando el prompt refinado"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nFT1wFHDWN3O"},"outputs":[],"source":["refined_summary_prompt_template = PromptTemplate(\n","    template=refined_prompt_str,\n","    input_variables=[\"text\"]\n",")\n","summary_chain = LLMChain(llm=llm_summary, prompt=refined_summary_prompt_template)\n","\n","with open(\"financial_report.txt\", \"r\", encoding=\"utf-8\") as f:\n","    texto_extenso = f.read()\n","\n","resumen = summary_chain.run(text=texto_extenso)\n","print(\"\\nResumen generado:\\n\")\n","display(Markdown(resumen))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1360,"status":"aborted","timestamp":1759814113044,"user":{"displayName":"VICTOR VALLEJO CARBALLO","userId":"16397145822283464326"},"user_tz":-120},"id":"AudulQhaa1wf"},"outputs":[],"source":["from IPython.display import Markdown\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"langchain.*\")\n","\n","\n","# Crear el prompt template\n","refine_prompt_template = PromptTemplate(\n","   template=refinement_template,\n","   input_variables=[\"base_prompt\"]\n",")\n","\n","\n","# Crear la cadena LLM con tu modelo\n","refine_chain = LLMChain(llm=llm_refine, prompt=refine_prompt_template)\n","\n","\n","# Ejecutar correctamente con invoke (nuevo estándar porque en versiones anteriores run() esta deprecado)\n","refined_prompt = refine_chain.invoke({\"base_prompt\": base_summary_prompt})\n","\n","\n","# Inspeccionar o extraer resultado (ajusta si la clave es diferente)\n","refined_prompt_str = refined_prompt[\"text\"] if \"text\" in refined_prompt else refined_prompt[\"base_prompt\"]\n","\n","\n","# Mostrar el resultado con estilo Markdown\n","print(\"Prompt refinado:\\n\")\n","display(Markdown(refined_prompt_str))\n"]},{"cell_type":"markdown","metadata":{"id":"MuOrMlZWWN3O"},"source":["- ¿Qué ventajas le ves a usar LangChain en vez de hacer todo a mano?\n","- ¿Te gusta más este resumen o el que hemos hecho antes? ¿Y si cambiamos la temperatura o el modelo?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CkE9-p4Hbdmn"},"outputs":[],"source":["refined_summary_prompt_template = PromptTemplate(\n","    template=refined_prompt[\"text\"],\n","    input_variables=[\"text\"]\n",")\n","summary_chain = LLMChain(llm=llm_summary, prompt=refined_summary_prompt_template)\n","\n","with open(\"financial_report.txt\", \"r\", encoding=\"utf-8\") as f:\n","    texto_extenso = f.read()\n","\n","resumen = summary_chain.run(text=texto_extenso)\n","print(\"\\nResumen generado:\\n\")\n","display(Markdown(resumen))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFq8d-6TbeIF"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":0}